\section{Introduction}


Brain-computer interface (BCI) systems establish a direct communcative link between the brain and an electronic system \cite{dornhege_toward_2007,mcfarland_brain-computer_2011}.  Recently, the combination of machine learning algorithms and non-invasive electroencephelographs (EEG) has yielded proof-of-concept systems ranging from brain-controlled keyboards and wheelchairs to prosthetic arms and hands \cite{blankertz_note_2007,millan_combining_2010,d._mattia_brain_2011,hill_practical_2014,campbell_neurophone:_2010}. 
% TODO: more sites on cool EEG applications?

There are a few reasons why these systems have not been widely adopted outside of lab settings. For one, they require large, complex scanning caps, which are impractical for disabled users and generally undesirable for ergonomic reasons \cite{ekandem_evaluating_2012,leeb_transferring_2013}. Meanwhile, the amount of data produced by these caps is large, requires significant processing, and are unsuited to mobile computing environments, in which these systems will most likely be deployed (e.g., smartphones, watches, embedded systems in scanning devices, etc). 

For BCI systems to enjoy wider adoption ``in the wild,'' they must calibrate to individual users quickly and achieve decent information transfer rates (ITR), but with fewer sensors than their lab-based counterparts, and with noisier signals, given that data acquisition should occur when people are performing daily tasks, such as moving, walking, talking, etc. As an added challenge, their computational firepower may be limited by the mobile \& wearable computing architectures on which they will must likely be deployed.

Do we really need dense, high-dimensional EEG data to achieve acceptable accuracy? In this study, we use recordings from a consumer-grade single, dry electroencephalographic (EEG) sensor to interrogate the efficacy of a signal extraction technique based on the logarithmic binning of power spectra over time \textcolor{red}{\bf [These should be presented as a novel technique, and shortly summarize its underpinning]}. To wit, we find that this technique significantly increases the calibration speed without a significant detriment to classification accuracy. Furthermore, it greatly decreases the size of feature vectors, making them easier to store in memory or to transmit through the web. \textcolor{red}{[the end is a little technical. It should outline the whole method, the results, and how it helps improve BCI (compared to other methods)]}
