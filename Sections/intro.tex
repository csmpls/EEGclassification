\section{Introduction}

Brain-computer interface (BCI) refers to the generation of machine-interpretable signals by brain signals, unmediated by musclar or nervous activity. Electroencephalography (EEG) is a popular mechanism for acheiving BCI, as its sensors are inexpensive, portable and non-invasive. However, EEG has limited spatial resolution, high variability between subjects and trials, and the "dry" electrodes suitable for everyday use yield poor signal quality. In order to compensate for these shortcomings, recent work has leveraged machine learning techniques to classify EEG data, and early successes with this technique have yielded brain-controlled keyboards [hex-o-spell], wheelchairs [millan], and prosthetic arms and hands [tobi].

BCI applications often require high electrode density and high temporal resolution, requiring dense and high-dimensional feature vectors, which leads to slow performance at both training and testing time. [] `something about how overfitting is a risk too` This classification bottleneck threatens the responsiveness of BCI applications to their users and places high requirements on end-users' hardware.

We expect that sensor resolution (and thus dimensionality) will continue to increase amdownstre, as will the sophistication of BCI classifiers, so we must seek another route to performant online classification of EEG data. One way to increse performance is to decrease the size of feature vectors by compressing preprocessed EEG data before training and classification, but little work has systematically examined how BCI applications could leverage compression to increase the performance of ML-base classification.

The present study seeks to examine fundamental tradeoffs between the size of feature vectors and classification accuracy in the context of a system that distinguishes between two mental tasks within-subject. Specifically, we examine the effect of binning frequency data on classification accuracy, and on the performance of our classifier at training and testing time. Most broadly, this study aims to create a tool that finds the optimal compromise between classification accuracy and speed/performance by iteratively adjusting the size of the feature vector.  